{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IOGS Machine Learning course - Neural Networks\n",
        "\n",
        "## Objective\n",
        "\n",
        "The objectives of this practical session:\n",
        "* introduction to neural network library PyTorch\n",
        "* the creation of a simple neural network for classification\n",
        "* implementation or usage of metrics and visualization tools to evaluate the performance of the training\n",
        "\n",
        "It includes the implementation of:\n",
        "* the neural network\n",
        "* the optimization loop\n",
        "* the test and evaluation"
      ],
      "metadata": {
        "id": "bdsdoUtsiDfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for the practical session\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm, tnrange"
      ],
      "metadata": {
        "id": "MxsgG80AGktn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language and libraries\n",
        "\n",
        "It is a first contact with [Pytorch](https://pytorch.org/), one of the mainly used Deep Learning frameworks (along with TensorFlow).\n",
        "\n",
        "We use it as tensor library. Note that in this session, Numpy could be sufficient to do everything.\n",
        "\n",
        "\n",
        "PyTorch implements a tensor library, mathematical functions, deep learning layers and utilities for designing and learning complex models.\n",
        "While most of the following practical session could be coded using Numpy, we will focus on PyTorch for several reasons:\n",
        "- we can switch to GPU computation if needed (parallel operations on GPU make computation more efficient, not needed for this course)\n",
        "- a lot of optimizers, dataloaders, layers are already coded, which will (in the end, after familiarization with the library) fast coding for deep learning models\n",
        "- coding with at least one deep learning framework is a common job/position requirement\n",
        "\n",
        "The documentation of PyTorch is your friend [here](https://pytorch.org/docs/stable/index.html).\n",
        "\n",
        "<font color='red'>Question</font>: Understand the two version of the same code, one using Numpy, the other using PyTorch"
      ],
      "metadata": {
        "id": "VH6r3x17DuLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NUMPY ----\")\n",
        "data_np = np.ones((10,7), dtype=np.float32) # defintion of a Matrix shape (10,7) with ones inside\n",
        "data_np[0] = 0 # set first line to 0\n",
        "data_np[5:7, 2:3] = 5 # slicing to set the values\n",
        "print(data_np)\n",
        "print(np.tanh(data_np)) # call a function\n",
        "data_np = np.expand_dims(data_np, axis=2) # adding a dimension\n",
        "print(data_np.shape)\n",
        "# convert from float to long int\n",
        "data_np = data_np.astype(np.int64)\n",
        "# convert from int to float\n",
        "data_np = data_np.astype(np.float32)\n",
        "\n",
        "print(\"PYTORCH ----\")\n",
        "data_th = torch.ones((10,7), dtype=torch.float)\n",
        "data_th[0] = 0\n",
        "data_th[5:7, 2:3] = 5\n",
        "print(data_th)\n",
        "print(torch.tanh(data_th)) # call a function\n",
        "data_th = data_th.unsqueeze(2) # adding a dimension\n",
        "print(data_th.shape)\n",
        "# convert from float to int\n",
        "data_th = data_th.long()\n",
        "# convert from int to float\n",
        "data_th = data_th.float()\n",
        "\n",
        "# conversion from PyTorch to Numpy\n",
        "data = data_th.numpy()\n",
        "\n",
        "# covnersion from Numpy to PyTorch\n",
        "data = torch.tensor(data_np)"
      ],
      "metadata": {
        "id": "YZrq_r4hGNPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data generation functions\n",
        "\n",
        "We will create three function to generate different type of data.\n",
        "The data is two numpy arrays, type `np.float`: the input points of shape `(npts, dim)`, and $y$ the labels for each point, shape `(npts, 1)`.\n",
        "\n",
        "We will generate random points in $[-1,1] \\times [-1,1]$.\n",
        "\n",
        "1. **Linear classification**: $y = 1$ if $x_1 + x_2 > 0$, $y=0$ otherwise\n",
        "2. **Bar classification**: $y==1$ if $0.25<x<0.75$\n",
        "2. **Donut classification**: $y = 1$ if $0.09<(x_1)^2 + (x_2)^2 < 0.49$, $y=0$ otherwise\n",
        "\n",
        "<font color='red'>Question</font> convert the commented numpy function for linear data to PyTorch.\n",
        "\n",
        "**Note:** visualization code is provided (and commented), you can uncomment the code to assess your code is good."
      ],
      "metadata": {
        "id": "xFniHQ6djEEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear\n",
        "# def generate_data_linear(npts, dim=2):\n",
        "#     data = np.random.rand(npts, dim)\n",
        "#     target = ((data[:,0] + data[:,1]) > 1).astype(int)\n",
        "#     return data, target\n",
        "\n",
        "def generate_data_linear(npts, dim=2): # Torch version\n",
        "    # code goes here\n",
        "    pass\n",
        "\n",
        "# visualization function\n",
        "def visualization_simple(data, target):\n",
        "    data_np = data.numpy()\n",
        "    target_np = target.numpy()\n",
        "    fig = plt.figure()\n",
        "    fig.set_size_inches(5, 5)\n",
        "    plt.scatter(data_np[:,0], data_np[:,1], c=target_np)\n",
        "\n",
        "# visualization of groundtruth and predictions\n",
        "def visualization_gt_pred(data, predictions, target):\n",
        "    data_np = data.numpy()\n",
        "    pred_np = predictions.numpy()\n",
        "    target_np = target.numpy()\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "    fig.set_size_inches(11, 5)\n",
        "    ax1.scatter(data_np[:,0], data_np[:,1], c=target_np)\n",
        "    ax2.scatter(data_np[:,0], data_np[:,1], c=pred_np)\n",
        "\n",
        "# visu linear\n",
        "# data, target = generate_data_linear(1000)\n",
        "# visualization_simple(data, target)"
      ],
      "metadata": {
        "id": "snJDpW9bRqXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>Question</font>: fill the functions for bar labels and donut labels, generate data and display."
      ],
      "metadata": {
        "id": "rZaJf0dmLPPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question: generete data bar\n",
        "def generate_data_bar(npts, dim=2):\n",
        "    # code goes here\n",
        "    pass\n",
        "\n",
        "# Question: generate data donut\n",
        "def generate_data_donut(npts, dim=2):\n",
        "    # code goes here\n",
        "    pass\n",
        "\n",
        "# visu bar\n",
        "# code goes here\n",
        "\n",
        "# visu donut\n",
        "# code goes here\n"
      ],
      "metadata": {
        "id": "kWz8_jxPKR_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network definition\n",
        "\n",
        "<font color='red'>Question</font> study the following code and comment it.\n",
        "\n",
        "<font color='red'>Question</font> run the code and comment the output."
      ],
      "metadata": {
        "id": "uCfTBHMzRjMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the network\n",
        "class NetworkExample(torch.nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.l1 = torch.nn.Linear(2,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        y = self.l1(x)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "train_pts, train_labels = generate_data_linear(5000)\n",
        "test_pts, test_labels = generate_data_linear(1000)\n",
        "\n",
        "num_epoch = 10\n",
        "\n",
        "net = NetworkExample()\n",
        "\n",
        "lr = 1e-2\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "losses = []\n",
        "iterations = []\n",
        "total_iter_counter = 0\n",
        "for epoch in range(num_epoch):\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    t = tqdm(torch.randperm(train_pts.shape[0]))\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    epoch_iter_counter = 0\n",
        "\n",
        "    for i in t:\n",
        "        \n",
        "        x = train_pts[i].reshape(1, 2)\n",
        "        target = train_labels[i].reshape(1,1).float()\n",
        "        \n",
        "        y2 = net(x)\n",
        "\n",
        "        loss = criterion(y2, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        epoch_iter_counter += 1\n",
        "\n",
        "        t.set_description_str(f\"Loss={total_loss/epoch_iter_counter:.4e}\")\n",
        "\n",
        "        total_iter_counter += 1\n",
        "    \n",
        "    total_loss /= train_pts.shape[0]\n",
        "    losses.append(total_loss)\n",
        "    iterations.append(total_iter_counter)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(iterations, losses)\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    predicted_logits = net(test_pts)\n",
        "    predicted_probas = torch.sigmoid(predicted_logits)\n",
        "    predicted_labels = (predicted_probas > 0.5).long()\n",
        "\n",
        "visualization_gt_pred(test_pts, predicted_labels, test_labels)\n"
      ],
      "metadata": {
        "id": "sTcRgHVoSj8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different data\n",
        "\n",
        "\n",
        "<font color='red'>Question</font>: copy paste the previous code and change to bar data generation."
      ],
      "metadata": {
        "id": "ke5M6ATrCxgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy code here"
      ],
      "metadata": {
        "id": "rLDIJpb5ZHgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color='red'>Question</font>: comment the results, explain\n",
        "\n",
        "*Answer*: (double-click on the cell to edit)"
      ],
      "metadata": {
        "id": "8D9lgnyKDQaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'> Question </font>: Copy paste the previous code, and change the network definition to a three-linear-layer network. Activation function (between linear layers) are hyperbolic tangent functions. The parameters of the network are the input size, the hidden size, and the output size. Train the newly defined network with 1) the bar data, 2) the donut data.\n",
        "Comments on the choice of parameters."
      ],
      "metadata": {
        "id": "st7gkNWCDfsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy code here"
      ],
      "metadata": {
        "id": "9O3jX3AUaW88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch size\n",
        "\n",
        "<font color='red'>Question</font>: copy-paste previous and modify the code to use a mini-batch of size 16 instead of a single point."
      ],
      "metadata": {
        "id": "e5RtguH4PCrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy code here"
      ],
      "metadata": {
        "id": "A8-t7aU6PDIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding a momentum\n",
        "\n",
        "Depending on the batch size, the gradient descent may be unstable. \n",
        "\n",
        "One solution is to increase the batch size (see previous) but depending on the problem it may not be always possible.\n",
        "\n",
        "Another approach is to use **momentum** during the optimization. \n",
        "\n",
        "$$v_t = \\gamma v_{t-1} + (1-\\gamma) \\Delta w $$\n",
        "$$w_t = w_{t-1} + \\alpha v_t $$\n",
        "\n",
        "<font color='red'>Question</font>: what is the intuition behing momentum?\n",
        "\n",
        "*answer here (double click on the cell)*\n",
        "\n",
        "<font color='red'>Question</font>: copy-paste previous and modify the code to use momentum. In PyTorch, it is an optimizer parameter."
      ],
      "metadata": {
        "id": "X1aE6MlxTowY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy code here"
      ],
      "metadata": {
        "id": "XAZJDKISw3NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Influence of the batch and momentum\n",
        "\n",
        "<font color='red'>Question</font>: plot the loss curves for mini-batch 1, mini-batch 16 and mini-batch 16 with momentum. Comment the results."
      ],
      "metadata": {
        "id": "oFwZ--K_n2fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code here"
      ],
      "metadata": {
        "id": "7extnvLscKPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-label classification\n",
        "\n",
        "<font color='red'>Question</font>: we give a new data generation code for multilabel classification. Copy-paste the previous code and adapt it.\n",
        "\n",
        "<font color='red'>Question</font>: comment the results with respect with previous binary classification results/architecture/parameters."
      ],
      "metadata": {
        "id": "_Ijh_pqif-sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data_multi_class(npts, n_classes):\n",
        "    data = torch.rand((npts, 2))*2-1\n",
        "    target = torch.zeros(npts, dtype=torch.long)\n",
        "    for i in range(n_classes-1):\n",
        "        mask = torch.logical_and(\n",
        "                ((data[:,0])**2 + (data[:,1])**2) < (i+1)/(n_classes),\n",
        "                ((data[:,0])**2 + (data[:,1])**2) > i/(n_classes)\n",
        "                )\n",
        "        target[mask] = i+1\n",
        "        \n",
        "    return data, target\n",
        "\n",
        "data, target = generate_data_multi_class(1000, 4)\n",
        "visualization_simple(data, target)"
      ],
      "metadata": {
        "id": "evr0MUUegCWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy code here"
      ],
      "metadata": {
        "id": "0wHLSF5piHr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}